{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8090/nlp/parse not responding\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import json\n",
    "import numpy as np\n",
    "from gensim import models, corpora\n",
    "from LingPlus.wsd import TopicWSD\n",
    "from LingPlus import wsd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主題模型和詞義消岐\n",
    "\n",
    "這份專案嘗試以主題模型（topic model），計算一個待消岐詞的「脈絡詞」，與一個詞義的CWN例句中的脈絡詞，兩者之間的詞彙連結度。並用該連結度（一個機率值）分辨待消岐詞是哪一個CWN的詞義（sense）。實作過程分為以下幾個階段\n",
    "1. 以平衡語料庫訓練一個主題模型（主題數200）。\n",
    "2. 把每一個CWN裡面的例句都當成是topic model裡的一篇「文章」，推測出其主題分配。\n",
    "3. 選出每個例句中的目標詞（即使用該例句的詞彙）的鄰近3個詞，這些詞為該目標詞的「脈絡詞」。\n",
    "4. 比對待消岐詞的脈絡詞，和每個詞義的例句中的脈絡詞，兩者間在主題模型中的詞彙連結度（word association）。\n",
    "5. 待消岐詞的詞義分配，即是從和每個例句的詞彙連結度計算出來的詞義機率，在常規化（normalizae）後的數值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See derivation of probability distribution of sense [here](https://www.overleaf.com/read/gncxhcbcbmys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些測試數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CWN詞義中，我們挑選第2句例句作為測試句，其他例句為「參照句」。所以，若詞義無例句或只有1個例句的詞義，即不納入測試的例句。只有1個詞義的詞彙，雖然不需要詞義消岐，但仍納入測試範圍。\n",
    "另外，為了描述TopicWSD所預測的詞義，是否比隨機預測的詞義更「接近」（亦即雖然不是它預測的詞義，但已經有較高機率是），\n",
    "所以也計算該正確詞義在預測詞義中的排序。如果詞義消岐愈正確，詞義排序的值應該愈小（排序0即代表預測詞義也是正確的詞義）。\n",
    "\n",
    "以下結果可詮釋為，如果待消岐句和例句的脈絡詞非常（甚至完全）相似，則TopicWSD有74%的機會可以預測到正確詞義，且即便預測錯誤，正確詞義平均而言也是第二高的預測詞義（詞義排序為0.81）。相對地，若待消岐句的脈絡詞是例句中從未出現的，則TopicWSD只有略高於機率水準的36%可以正確預測詞義。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistics on seen items\n",
    "Total trial count: 2991 (sample 1000 lemmas)  \n",
    "n_random_correct: 893, score: 0.30, mean-rank: 3.55  \n",
    "n_twsd_correct: 2223, score: 0.74, mean-rank: 0.81  \n",
    "\n",
    "## statistics on new items\n",
    "Total trial count: 24098  \n",
    "\n",
    "### Complete linkage\n",
    "n_random_correct: 7241, score: 0.30, mean-rank: 4.12  \n",
    "n_twsd_correct: 8725, score: 0.36, mean-rank: 3.33  \n",
    "\n",
    "### Single linkage\n",
    "n_random_correct: 7279, score: 0.30, mean-rank: 4.12  \n",
    "n_twsd_correct: 8697, score: 0.36, mean-rank: 3.33  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_heads = []\n",
    "with open(\"tm_topics.txt\", \"r\", encoding=\"UTF-8\") as fin:\n",
    "    for ln in fin.readlines():\n",
    "        words = ln.split(\":\")[1][2:-2]        \n",
    "        words = [w.replace(\"'\", \"\").strip() for w in words.split(',')]\n",
    "        topic_heads.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = r\"E:\\Kyle\\LingNLP\\week8_data\"\n",
    "model_path = join(DATA_PATH, \"model/asbc5_200_gensim_pass20.model\")\n",
    "tm = models.LdaMulticore.load(model_path)\n",
    "tm.gamma_threshold = 1e-5\n",
    "dictionary = corpora.Dictionary.load(join(DATA_PATH, \"model/gensim_asbc.dict\"))\n",
    "\n",
    "CWN_EXAMPLE_PATH = join(DATA_PATH, \"cwn_example.json\")\n",
    "with open(CWN_EXAMPLE_PATH, \"r\", encoding=\"UTF-8\") as fin:\n",
    "    cwn = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd.preproc.set_Oceanus_Endpoint(\"http://140.112.147.120:8090/nlp/parse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twsd = TopicWSD(cwn, tm, dictionary)\n",
    "twsd.sense_alpha = 1\n",
    "twsd.HELDOUT_IDX = -1\n",
    "twsd.linkage = TopicWSD.Linkage.Single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An explaining example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disambiguate(intext):\n",
    "    token_list = wsd.preproc.segment(intext)\n",
    "    print(\"/\".join([x[0] for x in token_list]))\n",
    "\n",
    "    for tok_i, tok in enumerate(token_list):\n",
    "        wsd_data = wsd.preproc.preprocess(token_list, tok_i)\n",
    "        lemma = tok[0]\n",
    "        if twsd.has_lemma(lemma):\n",
    "            print(\"--- %s ---\" % lemma)\n",
    "            psense = twsd.word_sense_disambiguate(wsd_data[1], wsd_data[0], lemma)            \n",
    "            \n",
    "            for sense_i in np.argsort([-x[1] for x in psense])[:5]:\n",
    "                sense_x = psense[sense_i]\n",
    "                print(\"[%s] %.4f: %s\" % (sense_x[0], sense_x[1], twsd.get_sense_def(lemma, sense_x[0])))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sense of 等:  8\n",
      "(13, ['守時', '是', '很', '重要', '的', '事', '。', '不', '能', '遲到', '，', '讓', '大家', '等', '你', '一', '個', '人', '。'])\n",
      "(1, ['我們', '等', '好久'])\n",
      "Ambig context:  ['我們', '好久']\n",
      "04001101\n",
      "Reference context: ['，', '讓', '大家', '你', '一', '個']\n",
      "Association prob:  0.0199639918751\n",
      "Reference context: ['一', '扔', '，', '看', '電視', '。']\n",
      "Association prob:  0.0065223789267\n",
      "Reference context: ['兩', '短', '」', '掛號', '、', '等']\n",
      "Association prob:  0.000274923395592\n",
      "04001102\n",
      "Reference context: ['分', '工作', '，', '錢', '存', '夠']\n",
      "Association prob:  0.00839079840987\n",
      "Reference context: ['他', '出去', '，', '天', '黑', '了']\n",
      "Association prob:  0.00651695967337\n",
      "Reference context: ['產下', '小熊', '，', '小熊', '滿', '一']\n",
      "Association prob:  0.00339941547101\n",
      "Reference context: ['兩', '年', '，', '念', '完', '了']\n",
      "Association prob:  0.0137128577981\n",
      "Reference context: ['一旁', '休息', '，', '天黑', '後', '，']\n",
      "Association prob:  0.00652270265882\n",
      "04001103\n",
      "Reference context: ['島', '的', '面積', '。']\n",
      "Association prob:  0.000493413707577\n",
      "Reference context: ['另', '增', '劃', '面積', '取代', '，']\n",
      "Association prob:  0.00183024768161\n",
      "Reference context: ['是', '一', '個', '的', '三角形', '，']\n",
      "Association prob:  2.69255452068e-06\n",
      "04001104\n",
      "Reference context: ['、', '丁', '四', '。']\n",
      "Association prob:  2.38031857404e-05\n",
      "Reference context: ['辣味', '分', '四', '，', '有', '輕']\n",
      "Association prob:  0.00029546243371\n",
      "Reference context: ['四', '類', '三', '給', '獎', '層次']\n",
      "Association prob:  0.00141382309401\n",
      "04001105\n",
      "Reference context: ['國際', '的', '一', '民族', '。']\n",
      "Association prob:  0.00550653764324\n",
      "Reference context: ['人', '是', '第一', '人', '，', '因為']\n",
      "Association prob:  0.00545854559054\n",
      "Reference context: ['粗略', '分為', '六', '星', '，', '最']\n",
      "Association prob:  0.00135829488623\n",
      "04001106\n",
      "Reference context: ['何忍', '看', '此', '事', '一再', '的']\n",
      "Association prob:  0.00100799632927\n",
      "Reference context: ['一反常態', '，', '以', '凌厲', '手法', '懲罰']\n",
      "Association prob:  0.00236287605078\n",
      "Reference context: ['需要', '辦理', '此', '調查', '之', '研究']\n",
      "Association prob:  0.0001019733764\n",
      "04001107\n",
      "Reference context: ['貴府', '上', '這', '人物', '沾親', '。']\n",
      "Association prob:  0.00109373879133\n",
      "Reference context: ['快', '那', '等', '，', '也', '會']\n",
      "Association prob:  0.019842543135\n",
      "Reference context: ['真', '有', '這', '人物', '！', '此']\n",
      "Association prob:  0.000899893306455\n",
      "04001108\n",
      "Reference context: ['過磷酸鈣', '，', '骨粉', '。', '鉀肥', '包括']\n",
      "Association prob:  7.62106910642e-05\n",
      "Reference context: ['與', '旅', '運費', '的', '預算', '編列']\n",
      "Association prob:  0.000790558042554\n",
      "Reference context: ['韓國', '、', '俄國', '七', '個', '國家']\n",
      "Association prob:  0.000897513862117\n",
      "Reference context: ['錢煦', '、', '林聖賢', '院士', '與', '陳維昭']\n",
      "Association prob:  0.0056069826459\n",
      "Reference context: ['﹑', '九', '如', '二', '里', '辦公處']\n",
      "Association prob:  0.00886694820083\n",
      "Reference context: ['、', '統計', '所', '十五', '個', '所']\n",
      "Association prob:  0.000191037403439\n",
      "我們/等/好久\n",
      "--- 等 ---\n",
      "[04001101] 0.2716: 維持現有狀態，直到所期望的對象或情況出現。\n",
      "[04001107] 0.2700: 人物的種類，強調其特殊性。\n",
      "[04001102] 0.1866: 引介所期待的對象或情況。\n",
      "[04001108] 0.1206: 表列舉清單結束。\n",
      "[04001105] 0.0749: 計算等級的單位。\n"
     ]
    }
   ],
   "source": [
    "lem = \"等\"\n",
    "ref_list = twsd.get_disambiguate_refdata(lem)\n",
    "ref_data = ref_list[0][1][0]\n",
    "ambig_data = wsd.preproc.preprocess(wsd.preproc.segment(\"我們等好久\"), 1)\n",
    "print(\"n_sense of 等: \", len(ref_list))\n",
    "print(ref_list[0][1][0])\n",
    "print(ambig_data)\n",
    "\n",
    "ambig_theta, ambig_word = twsd.get_anchor_topics(ambig_data[1], ambig_data[0])\n",
    "print(\"Ambig context: \", ambig_word)\n",
    "# compute compatability score\n",
    "import LingPlus.wsd.topic_utils as tu\n",
    "for senseid, examples in ref_list:\n",
    "    print(senseid)\n",
    "    for ref_data in examples:                \n",
    "        ref_theta, ref_word = twsd.get_anchor_topics(ref_data[1], ref_data[0])        \n",
    "        print(\"Reference context:\", ref_word)\n",
    "        if ref_word and ambig_word:\n",
    "            comp_score = tu.get_word_assoc(ambig_word, ref_word, ref_theta, \n",
    "                    twsd.model, twsd.dictionary)\n",
    "            print(\"Association prob: \", comp_score[0])\n",
    "\n",
    "disambiguate(\"我們等好久\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 討論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TopicWSD完全仰賴主題模型來建立例句和待消岐句之間的關係，所以主題模型的參數，\n",
    "例如主題數，詞彙前處理（尤其是去掉高低頻詞的程序），模型的hyperparameters，都可能影響最後的WSD結果。\n",
    "只是在CWN的資料裡，詞彙的詞義相當多，而每個詞義又只有3個左右的例句，可能較難以cross-validation的方式調校參數。\n",
    "\n",
    "2. 主題模型也是一種語義表徵的向量空間模型，而且其輸入材料基本上也是詞袋（bag of word）資料，詞的順序、詞類、句法等訊息是完全忽略的。\n",
    "有兩種可能的想法可以加入更多的語言訊息，例如（1）先將句子作句法剖析，用句法剖析來決定脈絡詞。例如透過dependency grammar，\n",
    "發現動詞「等」的主語或賓語，並用這兩個詞當作脈絡詞，或許能比現有作法更能含括到相關訊息；（2）不要用主題模型，而改用其他向量空間模型，\n",
    "並同時以詞彙共現與句法關係當作輸入，估計詞彙向量，並用該向量表徵計算待消岐句與例句的相似度；（3）用CWN本身的訊息加上句法結構作詞彙消岐，\n",
    "但可能會對資料的覆蓋率有其他的要求。\n",
    "\n",
    "3. TopicWSD在計算待消岐句與例句的詞彙關連度後，目前可以用Complete或Single linkage決定最後的詞義。沿襲階層群句法（hierarchical clustering）\n",
    "的定義，Complete linkage是以一個詞義中所有的例句，和待消岐句的詞彙關連度平均，當作該詞義最後與待消岐詞的機率；相對地，Single linkage是以一個詞義中\n",
    "機率最剛的那個例句，當作該詞義與待消岐句的機率。從實際應用的角度而言，一個詞義在CWN裡的例句可能各自是不同的脈絡，事實上只要有任一個例句\n",
    "和待消岐句的脈絡接近，它很可能就是屬於該詞義，而不需要真正符合所有該詞義中的所有例句的脈絡。所以在實用上，Single linkage可能是較實用的。\n",
    "\n",
    "4. 雖然在測試未知句子時，用uniform prior的正確率只有0.36。但實際上，並不是所有CWN列出來的詞義在真實語料中的出現機率都一致。\n",
    "甚至在CWN裡，排在前面的詞義「或許」反應其常用程度。故以詞義出現順序來調整先驗機率，可增加此方法在真實語料中的實用性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老師/說/不/能夠/打/同學\n",
      "--- 老師 ---\n",
      "[06667701] 1.0000: 具有教學能力或身份的人。\n",
      "--- 說 ---\n",
      "[05212412] 0.2206: 無義，表結束或停頓的語氣。\n",
      "[05212411] 0.2206: 無義，表後述單位為句賓。\n",
      "[05212406] 0.2198: 評價後述對象。\n",
      "[05212405] 0.2038: 指涉後述對象。\n",
      "[05212409] 0.0814: 說明特定事件的原因、理由使聽話者明白。\n",
      "--- 不 ---\n",
      "[05010901] 0.8096: 表疑問的語氣，置於句末。\n",
      "[04002001] 0.1904: 表否定，用在動詞、狀態形容詞與副詞前。\n",
      "[05010801] 0.0000: 不丹。\n",
      "--- 能夠 ---\n",
      "[05196302] 0.9988: 表具備達到後述事件成立的條件。\n",
      "[05196301] 0.0012: 形容具備完成特定事件能力的。\n",
      "[05196303] 0.0000: 表具有後述功能。\n",
      "--- 打 ---\n",
      "[052291b3] 0.0410: 發聲時，使舌頭快速振動。\n",
      "[05229124] 0.0410: 計算節拍以控制音樂進行的速度。\n",
      "[05229184] 0.0410: 拿比較熟悉的事物來描述、解釋另一事件。\n",
      "[05229178] 0.0410: 搭乘有座位的交通工具。\n",
      "[05229111] 0.0410: 比喻建立基礎。\n",
      "--- 同學 ---\n",
      "[06612801] 0.2987: 一起學習的人。\n",
      "[06612805] 0.2984: 在同一團體中一起進行特定事件的人。\n",
      "[06612803] 0.2976: 對學習者的稱呼。\n",
      "[06612804] 0.0576: 對不熟識的可能是在學年齡的對象的稱呼。\n",
      "[06612802] 0.0477: 一起學習。\n"
     ]
    }
   ],
   "source": [
    "disambiguate(\"老師說不能夠打同學\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "許多/遊客/為了/取景/拍照/直接/踏進/稻田/裡\n",
      "--- 許多 ---\n",
      "[05172701] 0.9879: 數量大或種類多。\n",
      "[05172702] 0.0121: 表前述特質的程度高。\n",
      "--- 遊客 ---\n",
      "[06604001] 1.0000: 前往特定地點休閒或娛樂的人。\n",
      "--- 為了 ---\n",
      "[07025201] 1.0000: 引介事件原因。\n",
      "--- 直接 ---\n",
      "[06772503] 0.8487: 表不避諱。\n",
      "[06772504] 0.1513: 形容表達方式直接的。\n",
      "[06772505] 0.0000: 形容比喻不必透過媒介就能產生關連的。\n",
      "[06772501] 0.0000: 表不改變方向向後述目的地運動。\n",
      "[06772506] 0.0000: 表不必透過其他媒介來做後述事件。\n",
      "--- 裡 ---\n",
      "[04086801] 0.6270: 事物內部不屬於表面的範圍。\n",
      "[04086802] 0.3683: 在前述對象所包含的空間範圍。\n",
      "[04086804] 0.0045: 在前述的機構或領域範圍。\n",
      "[04086803] 0.0001: 在前述的時間範圍內。\n"
     ]
    }
   ],
   "source": [
    "disambiguate(\"許多遊客為了取景拍照直接踏進稻田裡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "思考/必定/先/有/一/個/搞/清楚/的/問題/，/然後/多方/去/找/解答/，/比較/後/得出/一/個/可以/說服/自己/的/答案/。\n",
      "--- 必定 ---\n",
      "[05097001] 1.0000: 表對推論的肯定，沒有例外。\n",
      "--- 先 ---\n",
      "[06532401] 0.6292: 表時間或順序在前的。\n",
      "[07011801] 0.3053: 姓。\n",
      "[06532402] 0.0394: 處理時間或順序排在較前的特定對象。\n",
      "[06532404] 0.0218: 進行後述事件的時間比其他特定對象早。\n",
      "[06532405] 0.0040: 特定民族或宗族的上代。\n",
      "--- 有 ---\n",
      "WARNING: cannot parse punct(思-22, --21)\n",
      "[04013807] 0.8084: 表肯定事件的發生。\n",
      "[04013802] 0.1177: 斷定特定對象有後述性質。\n",
      "[04013803] 0.0570: 後述對象存在。\n",
      "[06036901] 0.0083: 給予對方的回應。\n",
      "[04013805] 0.0066: 達到後述程度。\n",
      "--- 一 ---\n",
      "[06727304] 0.3609: 表完成後述動作後，緊接著發生相關事件結果。\n",
      "[05224201] 0.2834: 數目字。整數，最小的正整數。\n",
      "[06727401] 0.2153: 姓。\n",
      "[05224202] 0.0639: 序數。順序排在第一。\n",
      "[06727301] 0.0388: 事物的全體。\n",
      "--- 個 ---\n",
      "[06533601] 0.4308: 人的身高。\n",
      "[07011204] 0.3612: 計算特定對象大約的數量的單位。\n",
      "[07011206] 0.1068: 表加強語氣，強調前述動作。\n",
      "[07011207] 0.0556: 表以輕鬆的態度提出進行特定事件的要求。\n",
      "[07011201] 0.0251: 計算能獨立成件的具體對象的單位。\n",
      "--- 清楚 ---\n",
      "[06664803] 0.8312: 清楚地知道。\n",
      "[06664801] 0.0889: 形容感官上明顯容易辨識。\n",
      "[06664802] 0.0799: 形容認知的內容容易明確。\n",
      "--- 的 ---\n",
      "[07023407] 0.6161: 表肯定或加強的語氣。\n",
      "[07023405] 0.3745: 代指後述省略的特定對象。\n",
      "[07023406] 0.0038: 表在前述情況下。\n",
      "[09003002] 0.0026: 表對推論的肯定。\n",
      "[07023409] 0.0005: 前述對象連接的是後述對象，表兩者指涉相同。\n",
      "--- 問題 ---\n",
      "[06560402] 0.5052: 需要被討論解決的困難。\n",
      "[06560401] 0.4836: 發問的內容。\n",
      "[06560405] 0.0111: 形容特定對象有需要被解決的困難，通常會製造麻煩的。\n",
      "[06560403] 0.0000: 要求對方回答以測試知識的敘述。\n",
      "[06560404] 0.0000: 不在預期中的負面狀況。\n",
      "--- 然後 ---\n",
      "[06667601] 1.0000: 表後述事件接在前述事件之後發生。\n",
      "--- 多方 ---\n",
      "[05065902] 1.0000: 形容多個相對或並列的對象。\n",
      "--- 去 ---\n",
      "[06559210] 0.4820: 以前述方式使特定對象變得不存在或改變擁有者。\n",
      "[06559217] 0.3459: 表反覆。\n",
      "[08052301] 0.1248: 咒罵語。\n",
      "[06559218] 0.0292: 比喻前述時段經過，成為特定時間點以前的時段。\n",
      "[06559202] 0.0070: 向離開參考點的方向移動。\n",
      "--- 找 ---\n",
      "[06558902] 0.9882: 比喻為了得到特定對象而努力尋找追求。\n",
      "[06558903] 0.0111: 和後述對象連繫以進行特定活動。\n",
      "[08010301] 0.0007: 退還超過應收金額的錢。\n",
      "[06558901] 0.0000: 搜尋需要的特定對象。\n",
      "--- 比較 ---\n",
      "[04148502] 1.0000: 表與其他同類對象做對照比較時，程度上高一點。\n",
      "--- 後 ---\n",
      "[03018215] 0.1922: 比喻實際狀況不容易看清，好像被表象所遮蓋的狀況。\n",
      "[03018217] 0.1920: 比喻在特定對象不在的場合。\n",
      "[03018206] 0.1908: 將有時間順序的事件分成兩部分，指在參考點之後的部分。\n",
      "[03018207] 0.1571: 在列舉的對象中，次序在後的對象。\n",
      "[03018216] 0.1263: 比喻特定對象在前述對象的成功中扮演未被宣揚的角色。\n",
      "--- 一 ---\n",
      "[05224201] 0.4914: 數目字。整數，最小的正整數。\n",
      "[06727302] 0.4574: 全體中的所有個體單位，強調各個個體都適用。\n",
      "[06727301] 0.0379: 事物的全體。\n",
      "[06727303] 0.0126: 表另外。\n",
      "[06727304] 0.0006: 表完成後述動作後，緊接著發生相關事件結果。\n",
      "--- 個 ---\n",
      "[07011202] 0.9722: 計算特定抽象名詞的單位。\n",
      "[07011204] 0.0232: 計算特定對象大約的數量的單位。\n",
      "[07011205] 0.0026: 表後述動作狀態。\n",
      "[06533602] 0.0014: 單獨一個的特定對象的單位。\n",
      "[07011201] 0.0003: 計算能獨立成件的具體對象的單位。\n",
      "--- 可以 ---\n",
      "[06715201] 1.0000: 表前述對象具有進行後述事件的條件。\n",
      "[06715203] 0.0000: 表程度高。\n",
      "[06715202] 0.0000: 表後述事件為聽話者可以選擇自己進行的事件之一。\n",
      "--- 自己 ---\n",
      "[05076902] 0.4127: 代指人對自身的意識與認知。\n",
      "[05076901] 0.3988: 代指主事者。\n",
      "[05076903] 0.1232: 緊鄰的前述對象，強調事件不牽涉其他的對象。\n",
      "[05076904] 0.0653: 代指自己所擁有的。\n",
      "[05076905] 0.0001: 形容控制情緒。\n",
      "--- 的 ---\n",
      "[07023407] 0.5129: 表肯定或加強的語氣。\n",
      "[07023405] 0.2907: 代指後述省略的特定對象。\n",
      "[09003002] 0.1905: 表對推論的肯定。\n",
      "[07023406] 0.0035: 表在前述情況下。\n",
      "[09003101] 0.0016: 箭靶的中心點。\n",
      "--- 答案 ---\n",
      "[06552503] 0.8567: 對測試知識的敘述的正確解答。\n",
      "[06552502] 0.1433: 對測試知識的敘述的所做的回答。\n"
     ]
    }
   ],
   "source": [
    "disambiguate(\"思考必定先有一個搞清楚的問題，然後多方去找解答，比較後得出一個可以說服自己的答案。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "網友/認為/其實/麵包店/土司/更貴/，/還有/人/抗議/為什麼/原物料/一直/上漲/，/薪水/卻/沒/漲/。\n",
      "--- 認為 ---\n",
      "[05201101] 1.0000: 引述或陳述前述對象的認知。\n",
      "--- 其實 ---\n",
      "[05100501] 1.0000: 表事件的實際情況。\n",
      "--- 還有 ---\n",
      "[04000401] 1.0000: 連接語意相似的詞組，表另外有的意思。\n",
      "--- 人 ---\n",
      "[05231109] 0.4607: 符合特定需求、從事特定工作的人。\n",
      "[05231107] 0.4129: 人的生理狀態。\n",
      "[05231111] 0.0855: 前述對象的跟隨者。\n",
      "[05231110] 0.0276: 人的本質特性。\n",
      "[05231105] 0.0128: 自己以外的人。\n",
      "--- 為什麼 ---\n",
      "[03044902] 1.0000: 事件的原因或理由，表疑問之意。\n",
      "[03044901] 0.0000: 表對事件的疑問，詢問原因。\n",
      "--- 一直 ---\n",
      "[06640105] 1.0000: 表事件狀態由前述描述發展到後述描述。\n",
      "[06640104] 0.0000: 表在後述時點前事件沒有改變。\n",
      "[06640101] 0.0000: 表不改變方向向後述目的地運動。\n",
      "[06640102] 0.0000: 表事件持續不間斷。\n",
      "[06640103] 0.0000: 表狀態從過去持續到參考時點，強調其狀態沒有改變過。\n",
      "--- 卻 ---\n",
      "[06614301] 1.0000: 表轉折。\n",
      "--- 沒 ---\n",
      "[03028101] 0.8250: 不擁有後述對象。\n",
      "[03028103] 0.0626: 未達一定的數量。\n",
      "[03028106] 0.0306: 表示疑問，置於句末。\n",
      "[03028104] 0.0280: 否定事件未達後述的程度。\n",
      "[03028102] 0.0274: 後述對象不存在。\n",
      "--- 漲 ---\n",
      "WARNING: cannot parse punct(變-5, ---13)\n",
      "[08024601] 0.9537: 使特定對象價格提高。\n",
      "[08024501] 0.0289: 形容水位上升。\n",
      "[06567702] 0.0149: 形容頭部充血。\n",
      "[06567701] 0.0025: 形容前述對象吸收水份後體積變大。\n",
      "[08024502] 0.0000: 形容比喻價格上升。\n"
     ]
    }
   ],
   "source": [
    "disambiguate(\"網友認為其實麵包店土司更貴，還有人抗議為什麼原物料一直上漲，薪水卻沒漲。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
